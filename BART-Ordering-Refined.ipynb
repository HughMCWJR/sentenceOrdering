{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52bda6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 12:57:46.844745: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 12:57:47.429247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 12:57:47.429296: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 12:57:47.429300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-07 12:57:48.318836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.319225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.319588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.319942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.320797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.321130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.321450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.321769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.322087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.322406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.322724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.323042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.323572: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 12:57:48.665638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.666005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.666334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.666661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.666992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.667312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.667639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.667961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.668281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.668601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.668923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:48.669242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.514527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.514923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.515276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.515639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.515981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.516315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.516646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.516971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.517301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.517635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 59 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-04-07 12:57:53.517960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.518281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1595 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "2023-04-07 12:57:53.518463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.518784: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 149 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:4b:00.0, compute capability: 7.5\n",
      "2023-04-07 12:57:53.518953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 12:57:53.519272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 2484 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:4c:00.0, compute capability: 7.5\n",
      "2023-04-07 12:57:53.531408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 59.75M (62652416 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2023-04-07 12:57:53.531972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 53.77M (56387328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    }
   ],
   "source": [
    "# Load BART summarizer/reorderer\n",
    "# FIGURE OUT HOW TO USE SPACEY TOKENIZATION WITH THIS?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941b128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reordering function using Bart\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Hyperparamers: MIN_TOKEN_MULTIPLIER, MAX_TOKEN_MULTIPLIER,\n",
    "# Log base in nDCG (or discounting function as a whole)\n",
    "\n",
    "# Trying shorter ordering\n",
    "#MIN_TOKEN_MULTIPLIER = 0.8\n",
    "#MAX_TOKEN_MULTIPLIER = 1\n",
    "\n",
    "# Gives better ordering? No, does not seem to give better ordering\n",
    "MIN_TOKEN_MULTIPLIER = 0.9\n",
    "MAX_TOKEN_MULTIPLIER = 1.1\n",
    "\n",
    "# Gets all sentences in output\n",
    "#MIN_TOKEN_MULTIPLIER = 1.1\n",
    "#MAX_TOKEN_MULTIPLIER = 1.3\n",
    "\n",
    "# Get number of tokens using nltk\n",
    "def getNumTokens(inputSentences):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    count = 0\n",
    "    for sentence in inputSentences:\n",
    "        count += len(tokenizer.tokenize(sentence))\n",
    "    return count\n",
    "\n",
    "# Takes in list of sentences and outputs reordered doc\n",
    "def reorder(inputSentences):\n",
    "    minLength = int(getNumTokens(inputSentences) * MIN_TOKEN_MULTIPLIER)\n",
    "    maxLength = int(getNumTokens(inputSentences) * MAX_TOKEN_MULTIPLIER)\n",
    "    if maxLength >= 1024:\n",
    "        raise Exception(\"Too long.\")\n",
    "    return summarizer(\" \".join(inputSentences), max_length=maxLength, min_length=minLength, do_sample=False)[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809585bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sentence tokenizer with spacy\n",
    "from functools import partial\n",
    "\n",
    "import spacy\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "spacy.prefer_gpu() # depending on whether you install CPU or GPU version\n",
    "\n",
    "def spacy_sentence_tokenizer(model: Language, text: str) -> list[str]:\n",
    "    doc = model(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf') # you need to download the gpu version of this model\n",
    "spacy_tokenizer = partial(spacy_sentence_tokenizer, nlp)\n",
    "#text = \"I am a Naman. I study at Auburn\"\n",
    "#sentences = spacy_tokenizer(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bc0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sem_nDCG Metric\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# Return list of sentences from string document\n",
    "def getSentences(doc):\n",
    "    return spacy_tokenizer(doc)\n",
    "\n",
    "# Add all possible adjacent sentence pairs to the end of the array\n",
    "def addSentencePairs(sentences):\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sentences.append(sentences[i] + \" \" + sentences[i + 1])\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode every sentence in list\n",
    "def getEncodings(sentences):\n",
    "    return [model.encode(sentence) for sentence in sentences]\n",
    "\n",
    "# Return list of lists of cosine similiarities where the similiarity between sentence i and j are at list[i][j]\n",
    "def getSimiliarities(correctSentenceEncodings, generatedSentenceEncodings):\n",
    "    similiarities = []\n",
    "    \n",
    "    for i in range(len(correctSentenceEncodings)):\n",
    "        similiarities.append([])\n",
    "        for j in range(0, len(generatedSentenceEncodings)):\n",
    "            similiarities[i].append(util.cos_sim(correctSentenceEncodings[i], generatedSentenceEncodings[j]))\n",
    "            \n",
    "    return similiarities\n",
    "\n",
    "# Get similiarity beteen sentences at indexes i and j in given similiarity data structure\n",
    "#def getSimScore(similiarities, i, j):\n",
    "#    if i == j:\n",
    "#        return None\n",
    "#    elif i < j:\n",
    "#        return similiarities[i][j-i]\n",
    "#    return similiarities[j][i-j]\n",
    "\n",
    "def getNumNonZeroes(twoDimArray):\n",
    "    count = 0\n",
    "    for x in range(len(twoDimArray)):\n",
    "        for y in range(len(twoDimArray[x])):\n",
    "            if twoDimArray[x][y] != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def getIfZero(twoDimArray):\n",
    "    for x in range(len(twoDimArray)):\n",
    "        for y in range(len(twoDimArray[x])):\n",
    "            if twoDimArray[x][y] != 0:\n",
    "                return False\n",
    "    return True    \n",
    "        \n",
    "# Get list of pairs of indexes, each pair is the most similiar pair found \n",
    "# up to that point wihtout repeating sentences\n",
    "# TO DO, change storage of encodings so that they are in order?\n",
    "def getBestPairings(similiarities):\n",
    "    pairs = []\n",
    "    sims = copy.copy(similiarities)\n",
    "    \n",
    "    #while getNumNonZeroes(sims) > 0:\n",
    "    while not getIfZero(sims):\n",
    "        maxScore = 0\n",
    "        bestPairIndexes = [0, 0]\n",
    "        \n",
    "        for i in range(len(sims)):\n",
    "            for j in range(len(sims[i])):\n",
    "                if sims[i][j] > maxScore:\n",
    "                    maxScore = sims[i][j]\n",
    "                    bestPairIndexes = [i, j]\n",
    "        \n",
    "        sims[bestPairIndexes[0]] = []\n",
    "        for k in range(len(sims)):\n",
    "            if sims[k] != []:\n",
    "                sims[k][bestPairIndexes[1]] = 0\n",
    "                            \n",
    "        pairs.append(bestPairIndexes)\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "# Reimplement functions to allow for matching adjacent sentences as a unit\n",
    "\n",
    "# Return list of sentences with adjacent ones combined\n",
    "def getSentencesWithCombinations(doc):\n",
    "    sentences = getSentences(doc)\n",
    "    newSentences = []\n",
    "    \n",
    "    for i in range(len(sentences) - 1):\n",
    "        newSentences.append(sentences[i])\n",
    "        newSentences.append(sentences[i] + \" \" + sentences[i + 1])\n",
    "        \n",
    "    newSentences.append(sentences[len(sentences) - 1])\n",
    "    \n",
    "    return newSentences\n",
    "\n",
    "# Get list of pairs of indexes, each pair is the most similiar pair found \n",
    "# up to that point without repeating sentences\n",
    "def getBestPairingsWithCombinations(similiarities):\n",
    "    pairs = []\n",
    "    sims = copy.copy(similiarities)\n",
    "    \n",
    "    #combinedCorrect = []\n",
    "    #combinedReordered = []\n",
    "    \n",
    "    while getNumNonZeroes(sims) > 0:\n",
    "        maxScore = 0\n",
    "        bestPairIndexes = [0, 0]\n",
    "        \n",
    "        for i in range(len(sims)):\n",
    "            for j in range(len(sims[i])):\n",
    "                if sims[i][j] > maxScore:\n",
    "                    maxScore = sims[i][j]\n",
    "                    bestPairIndexes = [i, j]\n",
    "                    \n",
    "        # Remove two adjacent indexes too bc if we add a combined sentence then\n",
    "        # its two sentences are used, and vice versa\n",
    "        #combinedCorrect.append(bestPairIndexes[0])\n",
    "        if bestPairIndexes[0] > 0:\n",
    "            sims[bestPairIndexes[0] - 1] = []\n",
    "        if bestPairIndexes[0] < len(sims) - 1:\n",
    "            sims[bestPairIndexes[0] + 1] = []\n",
    "        #if bestPairIndexes[1] % 2 == 1:\n",
    "            #combinedReordered.append(bestPairIndexes[1])\n",
    "            \n",
    "        sims[bestPairIndexes[0]] = []\n",
    "        for k in range(len(sims)):\n",
    "            if sims[k] != []:\n",
    "                sims[k][bestPairIndexes[1]] = 0\n",
    "                if bestPairIndexes[1] > 0:\n",
    "                    sims[k][bestPairIndexes[1] - 1] = 0\n",
    "                if bestPairIndexes[1] < len(sims[k]) - 1:\n",
    "                    sims[k][bestPairIndexes[1] + 1] = 0\n",
    "                            \n",
    "        pairs.append(bestPairIndexes)\n",
    "        \n",
    "    # Make corrections to pair indexes because of combined sentences\n",
    "    correctIndexes = []\n",
    "    reorderedIndexes = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        correctIndexes.append(pair[0])\n",
    "        reorderedIndexes.append(pair[1])\n",
    "        \n",
    "    sortedCorrectIndexes = sorted(correctIndexes)\n",
    "    sortedReorderedIndexes = sorted(reorderedIndexes)\n",
    "\n",
    "    numSkipped = 0\n",
    "    for i in range(len(sortedCorrectIndexes)):\n",
    "        pairs[correctIndexes.index(sortedCorrectIndexes[i])][0] = i + numSkipped\n",
    "        \n",
    "        if i != len(sortedCorrectIndexes) - 1:\n",
    "            if sortedCorrectIndexes[i] % 2 == 0:\n",
    "                if sortedCorrectIndexes[i + 1] - sortedCorrectIndexes[i] > 3:\n",
    "                    numSkipped += 1\n",
    "            else:\n",
    "                if sortedCorrectIndexes[i + 1] - sortedCorrectIndexes[i] > 4:\n",
    "                    numSkipped += 1\n",
    "                    \n",
    "    numSkipped = 0\n",
    "    for i in range(len(sortedReorderedIndexes)):\n",
    "        pairs[reorderedIndexes.index(sortedReorderedIndexes[i])][1] = i + numSkipped\n",
    "        \n",
    "        if i != len(sortedReorderedIndexes) - 1:\n",
    "            if sortedReorderedIndexes[i] % 2 == 0:\n",
    "                if sortedReorderedIndexes[i + 1] - sortedReorderedIndexes[i] > 3:\n",
    "                    numSkipped += 1\n",
    "            else:\n",
    "                if sortedReorderedIndexes[i + 1] - sortedReorderedIndexes[i] > 4:\n",
    "                    numSkipped += 1\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "# Output at 2d array where each one has the correct sentence first,\n",
    "# If correct sentence is missing, put (numberOfCorrectSentences - 1) for it\n",
    "def getOrderedPairs(pairs, numberOfCorrectSentences):\n",
    "    orderedPairs = []\n",
    "    \n",
    "    for i in range(numberOfCorrectSentences):\n",
    "        reorderedIndex = numberOfCorrectSentences - 1\n",
    "        found = False\n",
    "        \n",
    "        for j in range(len(pairs)):\n",
    "            if pairs[j][0] == i:\n",
    "                found = True\n",
    "                reorderedIndex = pairs[j][1]\n",
    "                break\n",
    "               \n",
    "        #if not found:\n",
    "            #print(\"Missed sentence \" + str(i))\n",
    "        orderedPairs.append([i, reorderedIndex])\n",
    "        \n",
    "    return orderedPairs\n",
    "\n",
    "# Get nDCG score for pairs\n",
    "# Uses log base 2\n",
    "# ^ Tinker with log to correctly balance importance of first sentence\n",
    "def nDCG(orderedPairs):\n",
    "    highestIndex = len(orderedPairs) - 1\n",
    "    \n",
    "    correctGains = [highestIndex - pair[0] for pair in orderedPairs]\n",
    "    reorderedGains = [highestIndex - pair[1] for pair in orderedPairs]\n",
    "    \n",
    "    numer = 0\n",
    "    denom = 0\n",
    "    \n",
    "    for i in range(len(orderedPairs)):\n",
    "        numer += reorderedGains[i] / math.log(2 + i, 2)\n",
    "        denom += correctGains[i] / math.log(2 + i, 2)\n",
    "        \n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9c05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentence pairing with combinations using Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6131073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/hugh/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d93ed7920cd404fbcaca203da62c46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get cnn_dailymail dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e8bc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 602, but you input_length is only 565. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=282)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "0.9205094194279998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 904, but you input_length is only 888. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=444)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 968, but you input_length is only 919. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=459)\n",
      "Your max_length is set to 536, but you input_length is only 532. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=266)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc too long\n",
      "Doc too long\n",
      "Doc too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test non-sentence pairing metric on cnn_dailymail dataset\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "trainDataset = dataset[\"train\"]\n",
    "\n",
    "file = open(\"bartResults.txt\", \"w\")\n",
    "file.write(\"\")\n",
    "file.close()\n",
    "\n",
    "for article in trainDataset:\n",
    "\n",
    "    sentences = getSentences(article[\"article\"])\n",
    "\n",
    "    correctDoc = \" \".join(sentences)\n",
    "\n",
    "    copyOfSentences = copy.copy(sentences)\n",
    "    random.shuffle(copyOfSentences)\n",
    "    shuffledSentences = copyOfSentences\n",
    "\n",
    "    try:\n",
    "        reorderedDoc = reorder(shuffledSentences)\n",
    "        print(\"Done ordering\")\n",
    "\n",
    "        \n",
    "    except:\n",
    "        print(\"Doc too long\")\n",
    "        continue\n",
    "        \n",
    "    #print(correctDoc + \"\\n\")\n",
    "    #print(\" \".join(shuffledSentences) + \"\\n\")\n",
    "    #print(reorderedDoc + \"\\n\")\n",
    "\n",
    "    correctSentences = getSentences(correctDoc)\n",
    "    reorderedSentences = getSentences(reorderedDoc)\n",
    "\n",
    "    correctEncodings = getEncodings(correctSentences)\n",
    "    reorderedEncodings = getEncodings(reorderedSentences)\n",
    "\n",
    "    simScores = getSimiliarities(correctEncodings, reorderedEncodings)\n",
    "    \n",
    "    bestPairs = getBestPairings(simScores)\n",
    "    \n",
    "    orderedPairs = getOrderedPairs(bestPairs, len(correctSentences))\n",
    "\n",
    "    # Metric output\n",
    "    result = nDCG(orderedPairs)\n",
    "    print(result)\n",
    "    file = open(\"bartResults.txt\", \"a\")\n",
    "    file.write(str(result) + \"\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06543e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encoderDecoder",
   "language": "python",
   "name": "encoderdecoder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
