{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4735a0fb",
   "metadata": {},
   "source": [
    "#from transformers import BartTokenizer, BartModel\n",
    "\n",
    "#tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "#model = BartModel.from_pretrained('facebook/bart-large')\n",
    "\n",
    "#inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "#output = model.generate(**inputs)\n",
    "#text = tokenizer.batch_decode(output)\n",
    "#print(text)n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52bda6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 10:59:37.339274: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 10:59:37.935020: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 10:59:37.935073: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 10:59:37.935077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-07 10:59:38.849638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.850107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.850518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.850941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.852154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.852551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.852928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.853314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.853687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.854062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.854433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.854826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:38.860013: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 10:59:39.303973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.304348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.304686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.305023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.305362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.305692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.306019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.306345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.306668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.306992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.307322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:39.307653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.327970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.328369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.328724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.329075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.329420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.329757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.330086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.330416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.330748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.331082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12432 MB memory:  -> device: 0, name: Quadro RTX 5000, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-04-07 10:59:44.331419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.331755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 2262 MB memory:  -> device: 1, name: Quadro RTX 5000, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "2023-04-07 10:59:44.331966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.332286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 591 MB memory:  -> device: 2, name: Quadro RTX 5000, pci bus id: 0000:4b:00.0, compute capability: 7.5\n",
      "2023-04-07 10:59:44.332468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-07 10:59:44.332788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 3151 MB memory:  -> device: 3, name: Quadro RTX 5000, pci bus id: 0000:4c:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Load BART summarizer/reorderer\n",
    "# FIGURE OUT HOW TO USE SPACEY TOKENIZATION WITH THIS?\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f62a65ca",
   "metadata": {},
   "source": [
    "# Summarization example\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a0f51c3",
   "metadata": {},
   "source": [
    "# Reordering example\n",
    "\n",
    "ARTICLE = \"\"\" But I went to sleep early because it was Monday night.\n",
    "When I got back from my walk I decided to try and leave work early because I was excited for game night.\n",
    "The day was a Monday and I woke up and went to work.\n",
    "After lunch I went for a walk. \n",
    "During lunch I ate a sandwich.\n",
    "We played Monopoly and had a blast.\n",
    "At 2 p.m. I saw a cat in the street on the way back to the office.\n",
    "Sam and Genesis arrived at 7 p.m. for games.\n",
    "\"\"\"\n",
    "numWords = ARTICLE.count(\" \")\n",
    "maxWords = int(numWords * 1.1)\n",
    "minWords = int(numWords * 0.9)\n",
    "print(summarizer(ARTICLE, max_length=maxWords, min_length=minWords, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "941b128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set reordering function using Bart\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Hyperparamers: MIN_TOKEN_MULTIPLIER, MAX_TOKEN_MULTIPLIER,\n",
    "# Log base in nDCG (or discounting function as a whole)\n",
    "\n",
    "# Trying shorter ordering\n",
    "#MIN_TOKEN_MULTIPLIER = 0.8\n",
    "#MAX_TOKEN_MULTIPLIER = 1\n",
    "\n",
    "# Gives better ordering? No, does not seem to give better ordering\n",
    "MIN_TOKEN_MULTIPLIER = 0.9\n",
    "MAX_TOKEN_MULTIPLIER = 1.1\n",
    "\n",
    "# Gets all sentences in output\n",
    "#MIN_TOKEN_MULTIPLIER = 1.1\n",
    "#MAX_TOKEN_MULTIPLIER = 1.3\n",
    "\n",
    "# Get number of tokens using nltk\n",
    "def getNumTokens(inputSentences):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    count = 0\n",
    "    for sentence in inputSentences:\n",
    "        count += len(tokenizer.tokenize(sentence))\n",
    "    return count\n",
    "\n",
    "# Takes in list of sentences and outputs reordered doc\n",
    "def reorder(inputSentences):\n",
    "    minLength = int(getNumTokens(inputSentences) * MIN_TOKEN_MULTIPLIER)\n",
    "    maxLength = int(getNumTokens(inputSentences) * MAX_TOKEN_MULTIPLIER)\n",
    "    if maxLength >= 1024:\n",
    "        raise Exception(\"Too long.\")\n",
    "    return summarizer(\" \".join(inputSentences), max_length=maxLength, min_length=minLength, do_sample=False)[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "809585bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sentence tokenizer with spacy\n",
    "from functools import partial\n",
    "\n",
    "import spacy\n",
    "\n",
    "from spacy.language import Language\n",
    "\n",
    "spacy.prefer_gpu() # depending on whether you install CPU or GPU version\n",
    "\n",
    "def spacy_sentence_tokenizer(model: Language, text: str) -> list[str]:\n",
    "    doc = model(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf') # you need to download the gpu version of this model\n",
    "spacy_tokenizer = partial(spacy_sentence_tokenizer, nlp)\n",
    "#text = \"I am a Naman. I study at Auburn\"\n",
    "#sentences = spacy_tokenizer(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bc0c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sem_nDCG Metric\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "# Return list of sentences from string document\n",
    "def getSentences(doc):\n",
    "    return spacy_tokenizer(doc)\n",
    "\n",
    "# Add all possible adjacent sentence pairs to the end of the array\n",
    "def addSentencePairs(sentences):\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sentences.append(sentences[i] + \" \" + sentences[i + 1])\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode every sentence in list\n",
    "def getEncodings(sentences):\n",
    "    return [model.encode(sentence) for sentence in sentences]\n",
    "\n",
    "# Return list of lists of cosine similiarities where the similiarity between sentence i and j are at list[i][j]\n",
    "def getSimiliarities(correctSentenceEncodings, generatedSentenceEncodings):\n",
    "    similiarities = []\n",
    "    \n",
    "    for i in range(len(correctSentenceEncodings)):\n",
    "        similiarities.append([])\n",
    "        for j in range(0, len(generatedSentenceEncodings)):\n",
    "            similiarities[i].append(util.cos_sim(correctSentenceEncodings[i], generatedSentenceEncodings[j]))\n",
    "            \n",
    "    return similiarities\n",
    "\n",
    "# Get similiarity beteen sentences at indexes i and j in given similiarity data structure\n",
    "#def getSimScore(similiarities, i, j):\n",
    "#    if i == j:\n",
    "#        return None\n",
    "#    elif i < j:\n",
    "#        return similiarities[i][j-i]\n",
    "#    return similiarities[j][i-j]\n",
    "\n",
    "def getNumNonZeroes(twoDimArray):\n",
    "    count = 0\n",
    "    for x in range(len(twoDimArray)):\n",
    "        for y in range(len(twoDimArray[x])):\n",
    "            if twoDimArray[x][y] != 0:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def getIfZero(twoDimArray):\n",
    "    for x in range(len(twoDimArray)):\n",
    "        for y in range(len(twoDimArray[x])):\n",
    "            if twoDimArray[x][y] != 0:\n",
    "                return False\n",
    "    return True    \n",
    "        \n",
    "# Get list of pairs of indexes, each pair is the most similiar pair found \n",
    "# up to that point wihtout repeating sentences\n",
    "# TO DO, change storage of encodings so that they are in order?\n",
    "def getBestPairings(similiarities):\n",
    "    pairs = []\n",
    "    sims = copy.copy(similiarities)\n",
    "    \n",
    "    #while getNumNonZeroes(sims) > 0:\n",
    "    while not getIfZero(sims):\n",
    "        maxScore = 0\n",
    "        bestPairIndexes = [0, 0]\n",
    "        \n",
    "        for i in range(len(sims)):\n",
    "            for j in range(len(sims[i])):\n",
    "                if sims[i][j] > maxScore:\n",
    "                    maxScore = sims[i][j]\n",
    "                    bestPairIndexes = [i, j]\n",
    "        \n",
    "        sims[bestPairIndexes[0]] = []\n",
    "        for k in range(len(sims)):\n",
    "            if sims[k] != []:\n",
    "                sims[k][bestPairIndexes[1]] = 0\n",
    "                            \n",
    "        pairs.append(bestPairIndexes)\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "# Reimplement functions to allow for matching adjacent sentences as a unit\n",
    "\n",
    "# Return list of sentences with adjacent ones combined\n",
    "def getSentencesWithCombinations(doc):\n",
    "    sentences = getSentences(doc)\n",
    "    newSentences = []\n",
    "    \n",
    "    for i in range(len(sentences) - 1):\n",
    "        newSentences.append(sentences[i])\n",
    "        newSentences.append(sentences[i] + \" \" + sentences[i + 1])\n",
    "        \n",
    "    newSentences.append(sentences[len(sentences) - 1])\n",
    "    \n",
    "    return newSentences\n",
    "\n",
    "# Get list of pairs of indexes, each pair is the most similiar pair found \n",
    "# up to that point without repeating sentences\n",
    "def getBestPairingsWithCombinations(similiarities):\n",
    "    pairs = []\n",
    "    sims = copy.copy(similiarities)\n",
    "    \n",
    "    #combinedCorrect = []\n",
    "    #combinedReordered = []\n",
    "    \n",
    "    while getNumNonZeroes(sims) > 0:\n",
    "        maxScore = 0\n",
    "        bestPairIndexes = [0, 0]\n",
    "        \n",
    "        for i in range(len(sims)):\n",
    "            for j in range(len(sims[i])):\n",
    "                if sims[i][j] > maxScore:\n",
    "                    maxScore = sims[i][j]\n",
    "                    bestPairIndexes = [i, j]\n",
    "                    \n",
    "        # Remove two adjacent indexes too bc if we add a combined sentence then\n",
    "        # its two sentences are used, and vice versa\n",
    "        #combinedCorrect.append(bestPairIndexes[0])\n",
    "        if bestPairIndexes[0] > 0:\n",
    "            sims[bestPairIndexes[0] - 1] = []\n",
    "        if bestPairIndexes[0] < len(sims) - 1:\n",
    "            sims[bestPairIndexes[0] + 1] = []\n",
    "        #if bestPairIndexes[1] % 2 == 1:\n",
    "            #combinedReordered.append(bestPairIndexes[1])\n",
    "            \n",
    "        sims[bestPairIndexes[0]] = []\n",
    "        for k in range(len(sims)):\n",
    "            if sims[k] != []:\n",
    "                sims[k][bestPairIndexes[1]] = 0\n",
    "                if bestPairIndexes[1] > 0:\n",
    "                    sims[k][bestPairIndexes[1] - 1] = 0\n",
    "                if bestPairIndexes[1] < len(sims[k]) - 1:\n",
    "                    sims[k][bestPairIndexes[1] + 1] = 0\n",
    "                            \n",
    "        pairs.append(bestPairIndexes)\n",
    "        \n",
    "    # Make corrections to pair indexes because of combined sentences\n",
    "    correctIndexes = []\n",
    "    reorderedIndexes = []\n",
    "    \n",
    "    for pair in pairs:\n",
    "        correctIndexes.append(pair[0])\n",
    "        reorderedIndexes.append(pair[1])\n",
    "        \n",
    "    sortedCorrectIndexes = sorted(correctIndexes)\n",
    "    sortedReorderedIndexes = sorted(reorderedIndexes)\n",
    "\n",
    "    numSkipped = 0\n",
    "    for i in range(len(sortedCorrectIndexes)):\n",
    "        pairs[correctIndexes.index(sortedCorrectIndexes[i])][0] = i + numSkipped\n",
    "        \n",
    "        if i != len(sortedCorrectIndexes) - 1:\n",
    "            if sortedCorrectIndexes[i] % 2 == 0:\n",
    "                if sortedCorrectIndexes[i + 1] - sortedCorrectIndexes[i] > 3:\n",
    "                    numSkipped += 1\n",
    "            else:\n",
    "                if sortedCorrectIndexes[i + 1] - sortedCorrectIndexes[i] > 4:\n",
    "                    numSkipped += 1\n",
    "                    \n",
    "    numSkipped = 0\n",
    "    for i in range(len(sortedReorderedIndexes)):\n",
    "        pairs[reorderedIndexes.index(sortedReorderedIndexes[i])][1] = i + numSkipped\n",
    "        \n",
    "        if i != len(sortedReorderedIndexes) - 1:\n",
    "            if sortedReorderedIndexes[i] % 2 == 0:\n",
    "                if sortedReorderedIndexes[i + 1] - sortedReorderedIndexes[i] > 3:\n",
    "                    numSkipped += 1\n",
    "            else:\n",
    "                if sortedReorderedIndexes[i + 1] - sortedReorderedIndexes[i] > 4:\n",
    "                    numSkipped += 1\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "# Output at 2d array where each one has the correct sentence first,\n",
    "# If correct sentence is missing, put (numberOfCorrectSentences - 1) for it\n",
    "def getOrderedPairs(pairs, numberOfCorrectSentences):\n",
    "    orderedPairs = []\n",
    "    \n",
    "    for i in range(numberOfCorrectSentences):\n",
    "        reorderedIndex = numberOfCorrectSentences - 1\n",
    "        found = False\n",
    "        \n",
    "        for j in range(len(pairs)):\n",
    "            if pairs[j][0] == i:\n",
    "                found = True\n",
    "                reorderedIndex = pairs[j][1]\n",
    "                break\n",
    "               \n",
    "        #if not found:\n",
    "            #print(\"Missed sentence \" + str(i))\n",
    "        orderedPairs.append([i, reorderedIndex])\n",
    "        \n",
    "    return orderedPairs\n",
    "\n",
    "# Get nDCG score for pairs\n",
    "# Uses log base 2\n",
    "# ^ Tinker with log to correctly balance importance of first sentence\n",
    "def nDCG(orderedPairs):\n",
    "    highestIndex = len(orderedPairs) - 1\n",
    "    \n",
    "    correctGains = [highestIndex - pair[0] for pair in orderedPairs]\n",
    "    reorderedGains = [highestIndex - pair[1] for pair in orderedPairs]\n",
    "    \n",
    "    numer = 0\n",
    "    denom = 0\n",
    "    \n",
    "    for i in range(len(orderedPairs)):\n",
    "        numer += reorderedGains[i] / math.log(2 + i, 2)\n",
    "        denom += correctGains[i] / math.log(2 + i, 2)\n",
    "        \n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3a5e669",
   "metadata": {},
   "source": [
    "# Test metric with sentence to sentence matching\n",
    "import random\n",
    "import copy\n",
    "\n",
    "sentences = [\"Monday morning, I woke and up and went to work.\",\n",
    "            \"I was not happy to get to work, but I needed to pay the bills.\",\n",
    "            \"One thing that helped was I was excited for game night.\",\n",
    "            \"But even this excitement was not enough to stop me from feeling dead by the time lunch rolled around.\",\n",
    "            \"I decided to quickly eat my meal so I could go on a walk.\",\n",
    "            \"While walking around, I saw a cat and a few pretty birds which helped my mood.\",\n",
    "            \"I then pushed myself to work really hard so I could leave work early.\",\n",
    "            \"When I got home I cleaned the house for game night.\",\n",
    "            \"My friends arrived that night and we had a blast.\",\n",
    "            \"We played for a few hours then we all got sleepy and they left.\",\n",
    "            \"Monday was a great day, I hope I have another one like it in the future.\"]\n",
    "\n",
    "correctDoc = \" \".join(sentences)\n",
    "\n",
    "copyOfSentences = copy.copy(sentences)\n",
    "random.shuffle(copyOfSentences)\n",
    "shuffledDoc = \" \".join(copyOfSentences)\n",
    "\n",
    "correctSentences = getSentences(correctDoc)\n",
    "shuffledSentences = getSentences(shuffledDoc)\n",
    "print(correctSentences)\n",
    "print(shuffledSentences)\n",
    "\n",
    "correctEncodings = getEncodings(correctSentences)\n",
    "shuffledEncodings = getEncodings(shuffledSentences)\n",
    "\n",
    "simScores = getSimiliarities(correctEncodings, shuffledEncodings)\n",
    "\n",
    "bestPairs = getBestPairings(simScores)\n",
    "\n",
    "for pair in bestPairs:\n",
    "    print(correctSentences[pair[0]])\n",
    "    print(shuffledSentences[pair[1]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e399e00",
   "metadata": {},
   "source": [
    "# Test metric with sentences that can be combined\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "sentences = [\"Monday morning, I woke and up and went to work.\",\n",
    "            \"I was not happy to get to work, but I needed to pay the bills.\",\n",
    "            \"One thing that helped was I was excited for game night.\",\n",
    "            \"But even this excitement was not enough to stop me from feeling dead by the time lunch rolled around.\",\n",
    "            \"I decided to quickly eat my meal so I could go on a walk.\",\n",
    "            \"While walking around, I saw a cat and a few pretty birds which helped my mood.\",\n",
    "            \"I then pushed myself to work really hard so I could leave work early.\",\n",
    "            \"When I got home I cleaned the house for game night.\",\n",
    "            \"My friends arrived that night and we had a blast.\",\n",
    "            \"We played for a few hours then we all got sleepy and they left.\",\n",
    "            \"Monday was a great day, I hope I have another one like it in the future.\"]\n",
    "\n",
    "correctDoc = \" \".join(sentences)\n",
    "\n",
    "copyOfSentences = copy.copy(sentences)\n",
    "random.shuffle(copyOfSentences)\n",
    "shuffledDoc = \" \".join(copyOfSentences)\n",
    "\n",
    "correctSentences = getSentences(correctDoc)\n",
    "shuffledSentences = getSentences(shuffledDoc)\n",
    "\n",
    "correctSentencesWithCombinations = getSentencesWithCombinations(correctDoc)\n",
    "shuffledSentencesWithCombinations = getSentencesWithCombinations(shuffledDoc)\n",
    "#print(correctSentences)\n",
    "#print(shuffledSentences)\n",
    "\n",
    "correctEncodings = getEncodings(correctSentencesWithCombinations)\n",
    "shuffledEncodings = getEncodings(shuffledSentencesWithCombinations)\n",
    "\n",
    "simScores = getSimiliarities(correctEncodings, shuffledEncodings)\n",
    "\n",
    "bestPairs = getBestPairingsWithCombinations(simScores)\n",
    "\n",
    "for pair in bestPairs:\n",
    "    print(correctSentences[pair[0]])\n",
    "    print(shuffledSentences[pair[1]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e9b6501",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Test sentence pairing on reordered sentences using Bart\n",
    "import random\n",
    "import copy\n",
    "\n",
    "#story = \"Upon graduating in September 1785, Bonaparte was commissioned a second lieutenant in La Fère artillery regiment. He served in Valence and Auxonne until after the outbreak of the French Revolution in 1789. Bonaparte was a fervent Corsican nationalist during this period. He asked for leave to join his mentor Pasquale Paoli, when Paoli was allowed to return to Corsica by the National Assembly. Paoli had no sympathy for Napoleon, however, as he deemed his father a traitor for having deserted his cause for Corsican independence.\"\n",
    "#story = \"Napoleon Bonaparte (born Napoleone Buonaparte; 15 August 1769 – 5 May 1821), later known by his regnal name Napoleon I, was a French military commander and political leader who rose to prominence during the French Revolution and led successful campaigns during the Revolutionary Wars. He was the de facto leader of the French Republic as First Consul from 1799 to 1804, then Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's political and cultural legacy endures to this day, as a highly celebrated and controversial leader. He initiated many liberal reforms that have persisted in society, and is considered one of the greatest military commanders in history. His wars and campaigns are studied by militaries all over the world. Between three and six million civilians and soldiers perished in what became known as the Napoleonic Wars.Napoleon was born on the island of Corsica, not long after its annexation by France, to a native family descending from minor Italian nobility. He supported the French Revolution in 1789 while serving in the French army, and tried to spread its ideals to his native Corsica. He rose rapidly in the Army after he saved the governing French Directory by firing on royalist insurgents. In 1796, he began a military campaign against the Austrians and their Italian allies, scoring decisive victories and becoming a national hero. Two years later, he led a military expedition to Egypt that served as a springboard to political power. He engineered a coup in November 1799 and became First Consul of the Republic.Differences with the United Kingdom meant France faced the War of the Third Coalition by 1805. Napoleon shattered this coalition with victories in the Ulm campaign, and at the Battle of Austerlitz, which led to the dissolution of the Holy Roman Empire. In 1806, the Fourth Coalition took up arms against him. Napoleon defeated Prussia at the battles of Jena and Auerstedt, marched the Grande Armée into Eastern Europe, and defeated the Russians in June 1807 at Friedland, forcing the defeated nations of the Fourth Coalition to accept the Treaties of Tilsit. Two years later, the Austrians challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle of Wagram.Hoping to extend the Continental System, his embargo against Britain, Napoleon invaded the Iberian Peninsula and declared his brother Joseph the King of Spain in 1808. The Spanish and the Portuguese revolted in the Peninsular War aided by a British army, culminating in defeat for Napoleon's marshals. Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic retreat of Napoleon's Grande Armée. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France, resulting in a large coalition army defeating Napoleon at the Battle of Leipzig. The coalition invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba, between Corsica and Italy. In France, the Bourbons were restored to power.Napoleon escaped in February 1815 and took control of France. The Allies responded by forming a Seventh Coalition, which defeated Napoleon at the Battle of Waterloo in June 1815. The British exiled him to the remote island of Saint Helena in the Atlantic, where he died in 1821 at the age of 51.Napoleon had an extensive impact on the modern world, bringing liberal reforms to the lands he conquered, especially the regions of the Low Countries, Switzerland and parts of modern Italy and Germany. He implemented many liberal policies in France and Western Europe.\"\n",
    "#story = \"A spokeswoman for the Manhattan district attorney’s office declined to comment. The news of the canceled session was first reported by Insider. While an indictment of Mr. Trump is not a certainty, prosecutors working for the Manhattan district attorney, Alvin L. Bragg, have signaled that charges are likely. They have been scrutinizing Mr. Trump for the hush-money payment that was made by his former fixer, Michael D. Cohen, in the run-up to the 2016 election. The timing of any potential indictment is unknown, and an arrest and arraignment — the criminal proceeding in which a defendant is formally charged — would not immediately follow. In order to indict Mr. Trump, Mr. Bragg’s prosecutors must ask the grand jury to vote whether to charge him. A majority of the 23 jurors must agree to do so. Once witness testimony has concluded, prosecutors are expected to explain any charges they are seeking to the jurors before asking them to vote. With the grand jury not meeting on Wednesday, the earliest that is likely to happen is Thursday afternoon. The charges likely center on the way Mr. Trump and his company, the Trump Organization, handled reimbursing Mr. Cohen for the payment of $130,000 to the porn star Stormy Daniels. The company’s internal records falsely identified the reimbursements as legal expenses, which helped conceal the purpose of the payments, according to Mr. Cohen, who said Mr. Trump knew about the misleading records. (Mr. Trump’s lawyers deny that and have accused Mr. Bragg’s office of targeting the former president for political purposes.\"\n",
    "story = \"The most merciful thing in the world, I think, is the inability of the human mind to correlate all its contents. We live on a placid island of ignorance in the midst of black seas of infinity, and it was not meant that we should voyage far. The sciences, each straining in its own direction, have hitherto harmed us little; but some day the piecing together of dissociated knowledge will open up such terrifying vistas of reality, and of our frightful position therein, that we shall either go mad from the revelation or flee from the deadly light into the peace and safety of a new dark age.      Theosophists have guessed at the awesome grandeur of the cosmic cycle wherein our world and human race form transient incidents. They have hinted at strange survivals in terms which would freeze the blood if not masked by a bland optimism. But it is not from them that there came the single glimpse of forbidden aeons which chills me when I think of it and maddens me when I dream of it. That glimpse, like all dread glimpses of truth, flashed out from an accidental piecing together of separated things—in this case an old newspaper item and the notes of a dead professor. I hope that no one else will accomplish this piecing out; certainly, if I live, I shall never knowingly supply a link in so hideous a chain. I think that the professor, too, intended to keep silent regarding the part he knew, and that he would have destroyed his notes had not sudden death seized him.      My knowledge of the thing began in the winter of 1926–27 with the death of my grand-uncle George Gammell Angell, Professor Emeritus of Semitic Languages in Brown University, Providence, Rhode Island. Professor Angell was widely known as an authority on ancient inscriptions, and had frequently been resorted to by the heads of prominent museums; so that his passing at the age of ninety-two may be recalled by many. Locally, interest was intensified by the obscurity of the cause of death. The professor had been stricken whilst returning from the Newport boat; falling suddenly, as witnesses said, after having been jostled by a nautical-looking negro who had come from one of the queer dark courts on the precipitous hillside which formed a short cut from the waterfront to the deceased’s home in Williams Street. Physicians were unable to find any visible disorder, but concluded after perplexed debate that some obscure lesion of the heart, induced by the brisk ascent of so steep a hill by so elderly a man, was responsible for the end. At the time I saw no reason to dissent from this dictum, but latterly I am inclined to wonder—and more than wonder.      As my grand-uncle’s heir and executor, for he died a childless widower, I was expected to go over his papers with some thoroughness; and for that purpose moved his entire set of files and boxes to my quarters in Boston. Much of the material which I correlated will be later published by the American Archaeological Society, but there was one box which I found exceedingly puzzling, and which I felt much averse from shewing to other eyes. It had been locked, and I did not find the key till it occurred to me to examine the personal ring which the professor carried always in his pocket. Then indeed I succeeded in opening it, but when I did so seemed only to be confronted by a greater and more closely locked barrier. For what could be the meaning of the queer clay bas-relief and the disjointed jottings, ramblings, and cuttings which I found? Had my uncle, in his latter years, become credulous of the most superficial impostures? I resolved to search out the eccentric sculptor responsible for this apparent disturbance of an old man’s peace of mind.\"\n",
    "\n",
    "#sentences = [\"Monday morning, I woke and up and went to work.\",\n",
    "#            \"I was not happy to get to work, but I needed to pay the bills.\",\n",
    "#            \"One thing that helped was I was excited for game night.\",\n",
    "#            \"But even this excitement was not enough to stop me from feeling dead by the time lunch rolled around.\",\n",
    "#            \"I decided to quickly eat my meal so I could go on a walk.\",\n",
    "#            \"While walking around, I saw a cat and a few pretty birds which helped my mood.\",\n",
    "#            \"I then pushed myself to work really hard so I could leave work early.\",\n",
    "#            \"When I got home I cleaned the house for game night.\",\n",
    "#            \"My friends arrived that night and we had a blast.\",\n",
    "#            \"We played for a few hours then we all got sleepy and they left.\",\n",
    "#            \"Monday was a great day, I hope I have another one like it in the future.\"]\n",
    "\n",
    "sentences = getSentences(story)\n",
    "\n",
    "correctDoc = \" \".join(sentences)\n",
    "\n",
    "copyOfSentences = copy.copy(sentences)\n",
    "random.shuffle(copyOfSentences)\n",
    "shuffledSentences = copyOfSentences\n",
    "\n",
    "reorderedDoc = reorder(shuffledSentences)\n",
    "\n",
    "print(correctDoc + \"\\n\")\n",
    "#print(\" \".join(shuffledSentences) + \"\\n\")\n",
    "print(reorderedDoc + \"\\n\")\n",
    "\n",
    "correctSentences = getSentences(correctDoc)\n",
    "reorderedSentences = getSentences(reorderedDoc)\n",
    "\n",
    "# TO DO thinking of only doing pairs for reordered sentences, not\n",
    "# sure if model combines sentences\n",
    "\n",
    "# Actually not sure if I should add sentence pairs to metric,\n",
    "# maybe I should if I was planning on use it as a loss\n",
    "# function of reinforcement learning\n",
    "#addSentencePairs(correctSentences)\n",
    "#addSentencePairs(reorderedSentences)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f626a3c9",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Continue testing on same story, now doing pairing and scoring\n",
    "\n",
    "correctEncodings = getEncodings(correctSentences)\n",
    "reorderedEncodings = getEncodings(reorderedSentences)\n",
    "\n",
    "simScores = getSimiliarities(correctEncodings, reorderedEncodings)\n",
    "\n",
    "bestPairs = getBestPairings(simScores)\n",
    "\n",
    "for pair in bestPairs:\n",
    "    print(pair)\n",
    "    print(correctSentences[pair[0]])\n",
    "    print(reorderedSentences[pair[1]])\n",
    "    \n",
    "orderedPairs = getOrderedPairs(bestPairs, len(correctSentences))\n",
    "\n",
    "print(orderedPairs)\n",
    "\n",
    "# Metric output\n",
    "print(nDCG(orderedPairs))\n",
    "\n",
    "# Best score\n",
    "#print(nDCG([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4]]))\n",
    "# Lowest? score\n",
    "#print(nDCG([[0, 4], [1, 3], [2, 2], [3, 1], [4, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9c05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentence pairing with combinations using Bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6131073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/hugh/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfc07c1a671470aa349139788f341dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get cnn_dailymail dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", '3.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e8bc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 602, but you input_length is only 565. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=282)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.897991496133948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 904, but you input_length is only 888. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=444)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.8362929679057544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 968, but you input_length is only 919. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=459)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7138423986988767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 536, but you input_length is only 532. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=266)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.9076701401354645\n",
      "Doc too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 907, but you input_length is only 886. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=443)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc too long\n",
      "Doc too long\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7644446961154083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 694, but you input_length is only 645. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=322)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.6923245046060423\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.839546016770027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1006, but you input_length is only 954. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=477)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7803863237299626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 811, but you input_length is only 789. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=394)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.5801814092430082\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7219663870076657\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7092648593668679\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.656757728301661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 984, but you input_length is only 924. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=462)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.6880105035626044\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.739698097638114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 564, but you input_length is only 563. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=281)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.8476392323924165\n",
      "Done ordering\n",
      "test1\n",
      "test2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 342, but you input_length is only 319. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=159)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7392565758589612\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7088226634188719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1020, but you input_length is only 982. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=491)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.8143734024212207\n",
      "Done ordering\n",
      "test1\n",
      "test2\n",
      "test3\n",
      "test4\n",
      "test5\n",
      "0.7005673017487287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 951, but you input_length is only 917. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=458)\n",
      "Your max_length is set to 225, but you input_length is only 223. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=111)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc too long\n",
      "Doc too long\n",
      "Doc too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 836, but you input_length is only 810. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=405)\n",
      "Your max_length is set to 314, but you input_length is only 300. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=150)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc too long\n",
      "Doc too long\n",
      "Doc too long\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test non-sentence pairing metric on cnn_dailymail dataset\n",
    "\n",
    "import random\n",
    "import copy\n",
    "\n",
    "trainDataset = dataset[\"train\"]\n",
    "\n",
    "file = open(\"bartResults.txt\", \"w\")\n",
    "file.write(\"\")\n",
    "file.close()\n",
    "\n",
    "for article in trainDataset:\n",
    "\n",
    "    sentences = getSentences(article[\"article\"])\n",
    "\n",
    "    correctDoc = \" \".join(sentences)\n",
    "\n",
    "    copyOfSentences = copy.copy(sentences)\n",
    "    random.shuffle(copyOfSentences)\n",
    "    shuffledSentences = copyOfSentences\n",
    "\n",
    "    try:\n",
    "        reorderedDoc = reorder(shuffledSentences)\n",
    "        print(\"Done ordering\")\n",
    "\n",
    "        #print(correctDoc + \"\\n\")\n",
    "        #print(\" \".join(shuffledSentences) + \"\\n\")\n",
    "        #print(reorderedDoc + \"\\n\")\n",
    "\n",
    "        correctSentences = getSentences(correctDoc)\n",
    "        reorderedSentences = getSentences(reorderedDoc)\n",
    "\n",
    "        correctEncodings = getEncodings(correctSentences)\n",
    "        reorderedEncodings = getEncodings(reorderedSentences)\n",
    "\n",
    "        simScores = getSimiliarities(correctEncodings, reorderedEncodings)\n",
    "\n",
    "        bestPairs = getBestPairings(simScores)\n",
    "\n",
    "        orderedPairs = getOrderedPairs(bestPairs, len(correctSentences))\n",
    "\n",
    "        # Metric output\n",
    "        result = nDCG(orderedPairs)\n",
    "        print(result)\n",
    "        file = open(\"bartResults.txt\", \"a\")\n",
    "        file.write(str(result) + \"\\n\")\n",
    "        file.close()\n",
    "    except:\n",
    "        print(\"Doc too long\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06543e58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "encoderDecoder",
   "language": "python",
   "name": "encoderdecoder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
